---
title: "2016 Election Analysis"
author: "Jiajia Zheng (231 level, Perm# 5090501), Tianyu Ren (231 level, Perm# 4544870)"
date: "December 4, 2018"
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: inline
---


# Instructions and Expectations

- You are allowed and encouraged to work with one partner on this project.  Include your names, perm numbers, and whether you are taking the class for 131 or 231 credit.

- You are welcome and encouraged to write up your report as a research paper (e.g. abstract, introduction, methods, results, conclusion) as long as you address each of the questions below.  Alternatively, you can format the assignment like a long homework by addressing each question in parts.

- There should be no raw R _output_ in the paper body!  All of your results should be formatted in a professional and visually appealing manner. That means, eather as a polished visualization or for tabular data, a nicely formatted table (see the documentation for [kable and kableExtra packages](https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf). If you feel you must include extensive raw R output, this should be included in an appendix, not the main report.  

- All R code should be available from your Rmarkdown file, but does not need to be shown in the body of the report!  Use the chunk option `echo=FALSE` to exclude code from appearing in your writeup.  In addition to your Rmarkdown, you should turn in the writuep as either a pdf document or an html file (both are acceptable).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

indent1 = '    '
indent2 = paste(rep(indent1, 2), collapse='')
indent3 = paste(rep(indent1, 3), collapse='')
doeval = TRUE
doecho = FALSE

library(knitr)
library(tidyverse)
library(kableExtra)
library(ggmap)
library(maps)
library(Rtsne)
library(NbClust)
library(tree)
library(maptree)
library(class)
library(glmnet)
library(ROCR)
library(randomForest)
library(e1071)
```

Predicting voter behavior is complicated for many reasons despite the tremendous effort in collecting, analyzing, and understanding many available datasets. 
For our final project, we will analyze the 2016 presidential election dataset.


# Background

The presidential election in 2012 did not come as a surprise. Some correctly predicted the outcome of the election correctly including [Nate Silver](https://en.wikipedia.org/wiki/Nate_Silver), 
and [many speculated his approach](https://www.theguardian.com/science/grrlscientist/2012/nov/08/nate-sliver-predict-us-election).

Despite the success in 2012, the 2016 presidential election came as a 
[big surprise](https://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/) 
to many, and it was a clear example that even the current state-of-the-art technology can surprise us.

Answer the following questions in one paragraph for each.


**1. What makes voter behavior prediction (and thus election forecasting) a hard problem?**  

Voter's behavior are influenced by the combined effects of race, wealth, unemployment rate and some sudden changes (economic, political, or random changes that are hard to measure), so it is inherentlly hard to predict. Additionally, variation between polling results due to sampling error could complicate the prediction even further. Some people give unhonest answers in polling, and supporters of a certain candidate tend to be more active in polling than those of the others. All these factors make voter behavior and the election hard to forecast.


**2. What was unique to Nate Silver's approach in 2012 that allowed him to achieve good predictions?**  

Instead of looking at the maximum probability, Silver looks at the full range of probabilities. To do this, Silver calculates a probability for each percentage support for Obama in each state, so he can use this data to ask how much of this probability is above 50%, in other words; what is the probability that Obama wins each state if the election was called on that day. This model can also be simulated forward in time to the election day for each estimated level of support (state and national). By weighting each forward simulation by the probability that the starting point is the true one, the model can be used to predict the probability that Obama would win the election. The maths behind this is calculated using Bayes' Theorem, plus a healthy dose of graph theory. From this, Silver can then calculate the new probabilities of each level of support.


**3. What went wrong in 2016? What do you think should be done to make future predictions better?**

Polling error leads to wrong prediction of the election results. It is due to statistical noise and some hard-to-quantify factors like nonresponse bias. The quality of polls in some states are poor - not enough, not recent enough or not by good enough pollsters. This brings more polling errors. The supporters of a certain candidate are just more enthusiastic in polling than the other's. All these errors added up and became larger than a candidate's winning margin so the election result came as a surprise. Specifically, the poll underestimated Trump's supporters in some Midwestern states, as well as some certain groups such as white men without higher education or some women.

To improve, there needs to be enough budget to allow for scientific polling, and incorporate more uncertainty measurement as opposed to just coming up with a single number. The polling needs to cover more representative groups of people to be less biased.



# Data

```{r data, message=FALSE, warning = FALSE}

## read data and convert candidate from string to factor
election.raw <- read_delim("data/election/election.csv", delim = ",") %>% mutate(candidate=as.factor(candidate))

census_meta <- read_delim("data/census/metadata.csv", delim = ";", col_names = FALSE) 
census <- read_delim("data/census/census.csv", delim = ",") 

```


## Election data

The meaning of each column in `election.raw` is clear except `fips`. The accronym is short for [Federal Information Processing Standard](https://en.wikipedia.org/wiki/FIPS_county_code).

In our dataset, `fips` values denote the area (US, state, or county) that each row of data represent. For example, `fips` value of 6037 denotes Los Angeles County.

```{r}
kable(election.raw %>% filter(county == "Los Angeles County"))  %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width=FALSE)
```

Some rows in `election.raw` are summary rows and these rows have `county` value of `NA`. There are two kinds of summary rows:

* Federal-level summary rows have `fips` value of `US`.
* State-level summary rows have names of each states as `fips` value.


**4. Report the dimension of `election.raw` after removing rows with `fips=2000`. Provide a reason for excluding them. Please make sure to use the same name `election.raw` before and after removing those observations. **

```{r, results = "hide"}

election.raw <- filter(election.raw, fips!="2000")
dim(election.raw)

```
The dimension of `election.raw` is 18345 * 5. 
The rows with `fips=2000` are the votes in Alaska. Since we already have the state summary votes of Alaska and it has no counties, it is better to remove them.


## Census data

Following is the first few rows of the `census` data:

```{r, echo=FALSE}
kable(census %>% head, "html")  %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width=FALSE) %>% scroll_box(width = "100%")
```

### Census data: column metadata

Column information is given in `metadata`.

```{r, dependson=data, echo=FALSE, eval=FALSE}
kable(census_meta)
```



## Data wrangling


**5. Remove summary rows from `election.raw` data: i.e.,**

* Federal-level summary into a `election_federal`.
    
* State-level summary into a `election_state`.
    
* Only county-level data is to be in `election`.

```{r}

election_federal <- filter(election.raw, fips=="US")
election_state <- filter(election.raw, is.na(county)==TRUE & fips!="US")
election <- filter(election.raw, is.na(county)==FALSE)

```


**6. How many named presidential candidates were there in the 2016 election? Draw a bar chart of all votes received by each candidate.  You can split this into multiple plots or may prefer to plot the results on a log scale.  Either way, the results should be clear and legible!**

```{r, results="hide"}

nrow(election_federal)
total_votes <- select(election_federal, c(candidate, votes))

```
There are 32 named presidential candidates in 2016 election. Below is the bar chart plot of the total count of votes reived by each candidate using log-scale at y-axis.


```{r, echo=FALSE}

par(mar=c(9,5,3,3))
barplot(total_votes$votes, 
        names.arg = total_votes$candidate, las = 2, cex.names = 0.8, 
        ylab = "Votes\n", log = "y", ylim = c(1, 10^8),
        main = "Total votes of each candidate")

```


**7. Create variables `county_winner` and `state_winner` by taking the candidate with the highest proportion of votes. **
  Hint: to create `county_winner`, start with `election`, group by `fips`, compute `total` votes, and `pct = votes/total`. 
  Then choose the highest row using `top_n` (variable `state_winner` is similar).

```{r, message=FALSE}

county_winner <- election %>%
  group_by(fips) %>%
  mutate(total = sum(votes)) %>%
  mutate(pct = votes/total) %>%
  arrange(desc(pct))  %>%
  top_n(1)

state_winner <- election %>%
  select(-county, -fips) %>% 
  group_by(state, candidate) %>%
  summarise(subtotal = sum(votes)) %>%
  mutate(pct = subtotal/sum(subtotal)) %>%
  arrange(desc(pct)) %>% 
  top_n(1)

```
    

# Visualization

Visualization is crucial for gaining insight and intuition during data mining. We will map our data onto maps.

The R package `ggplot2` can be used to draw maps. Consider the following code.

```{r, message=FALSE}

states <- map_data("state")

ggplot(data = states) + 
  geom_polygon(aes(x = long, y = lat, fill = region, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)  # color legend is unnecessary and takes too long

```

The variable `states` contain information to draw white polygons, and fill-colors are determined by `region`.


**8. Draw county-level map by creating `counties = map_data("county")`. Color by county**

```{r}

county <- map_data("county")

ggplot(data = county) + 
  geom_polygon(aes(x = long, y = lat, fill = subregion, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)  # color legend is unnecessary and takes too long

```


**9. Now color the map by the winning candidate for each state.**  First, combine `states` variable and `state_winner` we created earlier using `left_join()`. Note that `left_join()` needs to match up values of states to join the tables.  A call to `left_join()` takes all the values from the first table and looks for matches in the second table. If it finds a match, it adds the data from the second table; if not, it adds missing values:
  
```{r out.width="30%", fig.align="center", include=FALSE}
knitr::include_graphics("animated-left-join.gif")
```  
  
  
Here, we'll be combing the two datasets based on state name.  However, the state names are in different formats in the two tables: e.g. `AZ` vs. `arizona`. Before using `left_join()`, create a common column by creating a new column for `states` named `fips = state.abb[match(some_column, some_function(state.name))]`.  Replace `some_column` and `some_function` to complete creation of this new column. Then `left_join()`. Your figure will look similar to state_level [New York Times map](https://www.nytimes.com/elections/results/president).

```{r}

states <- mutate(states, fips = state.abb[match(states$region, tolower(state.name))])
stateWinnerMap <- left_join(states, state_winner, by = c("fips" = "state"))

```

```{r}

ggplot(data = stateWinnerMap) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)

```


**10. The variable `county` does not have `fips` column. So we will create one by pooling information from `maps::county.fips`.**
  Split the `polyname` column to `region` and `subregion`. Use `left_join()` combine `county.fips` into `county`. 
  Also, `left_join()` previously created variable `county_winner`. 
  Your figure will look similar to county-level [New York Times map](https://www.nytimes.com/elections/results/president).

```{r}

county.fips <- maps::county.fips %>% 
  separate(polyname, into = c("region", "subregion"), sep = ",")

county.fips.map <- left_join(county, county.fips, by = c("region", "subregion"))
county.fips.map$fips <- as.character(county.fips.map$fips)

countyWinnerMap <- left_join(county.fips.map, county_winner, by = "fips")

ggplot(data = countyWinnerMap) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)

```


**11. Create a visualization of your choice using `census` data.** Many exit polls noted that 
    [demographics played a big role in the election](https://fivethirtyeight.com/features/demographics-not-hacking-explain-the-election-results/).
    Use [this Washington Post article](https://www.washingtonpost.com/graphics/politics/2016-election/exit-polls/) 
    and [this R graph gallery](https://www.r-graph-gallery.com/) for ideas and inspiration.
    

```{r, echo=FALSE, warning=FALSE}

par(mfrow = c(2,1))

ggplot(data = census, aes(x = State, y = Income/1000)) +
  geom_boxplot(color = "royalblue") + 
  labs(x = "State", y = "Household Income ($1,000)") +
  ylim(0, 200) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

ggplot(data = census, aes(x = White, y = Unemployment)) +
  geom_point(color = "brown2") + 
  labs(x = "% White", y = "Unemployment Rate (%)") +
  ylim(0, 40) +
  geom_smooth(method=lm)

```

The above first plot shows the household income range of each state. People with different income level could vote for different candidates.
The second plot shows the correlation between percentage of white people and the unemployment rate. If an area has higher percentage of white people, it tends to have lower unemployment rate.


**12. The `census` data contains high resolution information (more fine-grained than county-level). In this problem, we aggregate the information into county-level data by computing `TotalPop`-weighted average of each attributes for each county. Create the following variables:**
    
* _Clean census data `census.del`_: 
      start with `census`, filter out any rows with missing values, 
      convert {`Men`, `Employed`, `Citizen`} attributes to percentages (meta data seems to be inaccurate), 
      compute `Minority` attribute by combining {Hispanic, Black, Native, Asian, Pacific}, remove these variables after creating `Minority`, remove {`Walk`, `PublicWork`, `Construction`}.  
      _Many columns seem to be related, and, if a set that adds up to 100%, one column will be deleted._  
      

* _Sub-county census data, `census.subct`_: 
      start with `census.del` from above, `group_by()` two attributes {`State`, `County`}, 
      use `add_tally()` to compute `CountyTotal`. Also, compute the weight by `TotalPop/CountyTotal`.
    

* _County census data, `census.ct`_: 
      start with `census.subct`, use `summarize_at()` to compute weighted sum
    

* _Print few rows of `census.ct`_: 
    
```{r}

census.del <- census[complete.cases(census[ , -1]),] %>%
  mutate(Men = round(Men/TotalPop*100, 1)) %>%
  mutate(Employed = round(Employed/TotalPop*100, 1)) %>%
  mutate(Citizen = round(Citizen/TotalPop*100, 1))

census.del <- mutate(census.del, Minority = rowSums(census.del[,c("Hispanic", "Black", "Native", "Asian", "Pacific")])) %>%
  select(-c("Hispanic", "Black", "Native", "Asian", "Pacific", "Walk", "PublicWork", "Construction", "Women"))

```

```{r}

census.subct <- census.del %>%
  group_by(State, County) %>%
  add_tally(TotalPop) %>%
  rename(CountyTotal = n) %>%
  mutate(Weight = round(TotalPop/CountyTotal*100, 1))

```

```{r}

census.ct <- census.subct %>%
  group_by(State, County) %>%
  summarise_at(vars(Men:Minority), funs(weighted.mean( ., Weight)))

```




# Dimensionality reduction

**13. Run PCA for both county & sub-county level data.** Save the first two principle components PC1 and PC2 into a two-column data frame, call it `ct.pc` and `subct.pc`, respectively. Discuss whether you chose to center and scale the features before running PCA and the reasons for your choice.  What are the three features with the largest absolute values of the first principal component? Which features have opposite signs and what does that mean about the correaltion between these features?

```{r}

pca.ct <- prcomp(census.ct[,-c(1:2)], center = TRUE, scale = TRUE)
pca.subct <- prcomp(census.subct[, -c(1:3, 30:31)], center = TRUE, scale = TRUE)

ct.pc <- pca.ct$x[, 1:2]
subct.pc <- pca.subct$x[, 1:2]

```
The numerical variables are measured on different scales, so we set `center = TRUE` and `scale = TRUE` to subtract the mean and divide the standard deviation from each feature. It is important in PCA to normalize the data so that we can maximize the variance. Without rescaling the predictors, PCA would perform poorly.


```{r}

kable(head(sort(abs(pca.ct$rotation[, 1]), decreasing = TRUE), 3), col.names = "Absolute value", "html", booktabs = T) %>% 
  kable_styling(position = "center", full_width = F)
kable(head(sort(abs(pca.subct$rotation[, 1]), decreasing = TRUE), 3), col.names = "Absolute value", "html", booktabs = T) %>% 
  kable_styling(position = "center", full_width = F)

```
The first three features with the largest absolute values of the 1st PC for county level data are *IncomePerCap*, *ChildPoverty* and *Poverty*. And those of sub-county level data are *IncomePerCap*, *Professional* and *Poverty*.


```{r, results="hide"}

pca.ct$rotation[,1]

```

 | IncomePerCap | ChildPoverty |   Poverty   |
 |--------------|--------------|-------------|
 | 0.351552609  | -0.343578774 | -0.342276879| 

For county level data, the variable *IncomePerCap* (0.35) has opposite sign with *ChildPoverty* (-0.34) and *Poverty* (-0.34). This means that *IncomePerCap* is negatively correlated with *ChildPoverty* and *Poverty*, while the latter two poverty-related variables are positively correlated.


```{r, results="hide"}

pca.subct$rotation[,1]

```

 | IncomePerCap | Professional |   Poverty  |
 |--------------|--------------|------------|
 | -0.31850095  | -0.30673775  | 0.30480845 | 

For sub-county level data, the variable *IncomePerCap* (-0.32) has same sign with *Professional* (-0.31) but opposite sign with *Poverty* (0.30). This implies that *IncomePerCap* has positive correlation with *Professional* but negative correlation with *Poverty*.


**14. Determine the number of minimum number of PCs needed to capture 90% of the variance for both the county and sub-county analyses.** Plot proportion of variance explained (PVE) and cumulative PVE for both county and sub-county analyses.

```{r, results="hide"}

sdev.ct <- pca.ct$sdev
pve.ct <- sdev.ct^2/sum(sdev.ct^2)
cum.pve.ct <- cumsum(pve.ct)

sdev.subct <- pca.subct$sdev
pve.subct <- sdev.subct^2/sum(sdev.subct^2)
cum.pve.subct <- cumsum(pve.subct)

min(which(cum.pve.ct > 0.9))
min(which(cum.pve.subct > 0.9))

```
For county level data, a minimum of 13 PCs are needed to capture 90% of the variance, and for sub-county level data at least 15 PCs are needed.

```{r}

par(mfrow=c(2, 2))

plot(pve.ct, type="l", lwd=3, xlab="Principal Component (county level)", ylab="PVE", ylim=c(0,1))
plot(cum.pve.ct, type="l", lwd=3, xlab="Principal Component (county level)", ylab="Cumulative PVE", ylim=c(0, 1))

plot(pve.subct, type="l", lwd=3, xlab="Principal Component (sub-county level)", ylab="PVE", ylim=c(0,1))
plot(cum.pve.subct, type="l", lwd=3, xlab="Principal Component (sub-county level)", ylab="Cumulative PVE", ylim=c(0, 1))

```

# Clustering

**15. With `census.ct`, perform hierarchical clustering with complete linkage.**  Cut the tree to partition the observations into 10 clusters. Re-run the hierarchical clustering algorithm using the first 2 principal components of `ct.pc` as inputs instead of the original features.  Compare and contrast the results. For both approaches investigate the cluster that contains San Mateo County. Which approach seemed to put San Mateo County in a more appropriate clusters? Comment on what you observe and discuss possible explanations for these observations.

```{r}

census.ct.scaled = scale(census.ct[, -c(1,2)], center = TRUE, scale = TRUE)

county.dis = dist(census.ct.scaled, method = "euclidean")

set.seed(111)

county.hc = hclust(county.dis, method = "complete")
county.clus = cutree(county.hc, 10)

kable(table(county.clus), col.names = c("Cluster", "Number of County"),  "html", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)


```


```{r}

county.pc2.dis = dist(ct.pc, method = "euclidean")

set.seed(222)

county.pc2.hc = hclust(county.pc2.dis, method = "complete")
county.pc2.clus = cutree(county.pc2.hc, 10)

kable(table(county.pc2.clus), col.names = c("Cluster", "Number of County"),  "html", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)

```

```{r, results="hide"}

county.clus[census.ct$County == "San Mateo"]
county.pc2.clus[census.ct$County == "San Mateo"]

```
Using complete linkage in hierarchical clustering, San Mateo County falls in the cluster 3 which contains 139 counties. If we use only the first two components as inputs, it falls in cluster 1 which contains 992 counties.


```{r, results="hide"}

summary(census.ct[county.clus == 3, ])

```

```{r, results="hide"}

summary(census.ct[county.pc2.clus == 1, ])

```

|  Method |        |IncomePerCap | ChildPoverty |
|---------|--------|-------------|--------------|
|    1    | Median |   $37202    |     9.8%     |
|         |   Max  |   $60993    |              |
|    2    | Median |   $28399    |    14.5%     |
|         |   Max  |   $51364    |              |
|San Maneo|  True  |   $47860    |     9.7%     |
|         |        |             |              |

Comparing the above two summary tables of the clusters that San Manteo belongs to using two different methods, we can conclude that the first clustering method is better. For example, the income per capita of San Mateo County is \$48K in the census data. If we look at the values of variable *IncomPerCap*, the first method yields a median value of \$37K with a maximum of \$61K while the latter method yields \$28K with a maximum of \$51K. Additionally, the child poverty rate of San Mateo County is 9.7%. The median value of variable *ChildPoverty* in the first method's results is 9.8% whereas in the second method's it is 14.5%. 
Therefore, the first method outperforms the second in terms of putting San Mateo County in the right cluster.  

# Classification

In order to train classification models, we need to combine `county_winner` and `census.ct` data.
This seemingly straightforward task is harder than it sounds. 
Following code makes necessary changes to merge them into `election.cl` for classification.

```{r}
tmpwinner <- county_winner %>% ungroup %>%
  mutate(state = state.name[match(state, state.abb)]) %>%               ## state abbreviations
  mutate_at(vars(state, county), tolower) %>%                           ## to all lowercase
  mutate(county = gsub(" county| columbia| city| parish", "", county))  ## remove suffixes
tmpcensus <- census.ct %>% ungroup %>% mutate_at(vars(State, County), tolower)


election.cl <- tmpwinner %>%
  left_join(tmpcensus, by = c("state"="State", "county"="County")) %>% 
  na.omit

election.cl2 = election.cl %>% select(-c(fips))
## save meta information
election.meta <- election.cl %>% select(c(county, fips, state, votes, pct, total))

## save predictors and class labels

election.cl = election.cl %>% select(-c(county, fips, state, votes, pct, total))

```

Using the following code, partition data into 80% training and 20% testing:
```{r}
set.seed(10) 
n <- nrow(election.cl)
in.trn <- sample.int(n, 0.8*n) 
trn.cl <- election.cl[ in.trn,]
tst.cl <- election.cl[-in.trn,]
```

Using the following code, define 10 cross-validation folds:
```{r}
set.seed(20) 
nfold <- 10
folds <- sample(cut(1:nrow(trn.cl), breaks=nfold, labels=FALSE))
```

Using the following error rate function:
```{r}
calc_error_rate = function(predicted.value, true.value){
  return(mean(true.value!=predicted.value))
}
records = matrix(NA, nrow=3, ncol=2)
colnames(records) = c("train.error","test.error")
rownames(records) = c("tree","logistic","lasso")
```

## Classification

**16. Decision tree: train a decision tree by `cv.tree()`.** Prune tree to minimize misclassification error. Be sure to use the `folds` from above for cross-validation. Visualize the trees before and after pruning. Save training and test errors to `records` variable. Intepret and discuss the results of the decision tree analysis. Use this plot to tell a story about voting behavior in the US (remember the [NYT infographic?](https://archive.nytimes.com/www.nytimes.com/imagepages/2008/04/16/us/20080416_OBAMA_GRAPHIC.html))

```{r}
#Tree prior pruning

tree.election = tree(candidate ~., data = trn.cl)
draw.tree(tree.election, nodeinfo=TRUE, cex=0.5)

#Tree cv
cv = cv.tree(tree.election, folds ,FUN = prune.misclass)
best.cv = cv$size[which.min(cv$dev)]
```


```{r}
# Prune tree.election
pt.prune = prune.misclass (tree.election, best=best.cv)
draw.tree(pt.prune, nodeinfo=TRUE, cex=0.5)


#Compute the training error
pred.pt.prune = predict(pt.prune, trn.cl, type="class")
err.pt.prune = table(factor(pred.pt.prune), t((select(trn.cl, candidate))))
 kable(err.pt.prune, "html", caption = "treeTrainError", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)

tree_train_error = (err.pt.prune[1,2] + err.pt.prune[2,1])/length(pred.pt.prune)

#Compute the test error


pred2.pt.prune = predict(pt.prune, tst.cl, type="class")
err.pt.prune2 = table(factor(pred2.pt.prune), t((select(tst.cl, candidate))))
 kable(err.pt.prune2, "html", caption = "treeTestError", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)


tree_test_error = (err.pt.prune2[1,2] + err.pt.prune2[2,1])/length(pred2.pt.prune)


records[1,1] = tree_train_error
records[1,2] = tree_test_error


```
  
  The first best decision tree we got has 12 different terminal nodes. It has a training error rate of 0.0724 and test error rate of 0.0977, which is very good in predicting the true outcomes of the election.
    We see that Donald Trump seems to be favored by demographics where there is a predominant white population, with relatively lower income per capita and production. While Hillary Clinton seems to be more favored by minorities and unemployed poeple.

**17. Run a logistic regression to predict the winning candidate in each county.**  Save training and test errors to `records` variable.  What are the significant variables? Are the consistent with what you saw in decision tree analysis? Interpret the meaning of a couple of the significant coefficients in terms of a unit change in the variables.  

```{r, message=FALSE, warning = FALSE}
glm.fit = glm(candidate ~ .,data=trn.cl, family=binomial)



#Find the training error 
prob.training = round(predict(glm.fit, type="response"), digits=2)


trainPrediction = as.factor(ifelse(prob.training<=0.5,  "Donald Trump", "Hillary Clinton"))
trainTable = table(trainPrediction, t((select(trn.cl, candidate))))

kable(trainTable, "html", caption = "logisticTrainTable", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)

logisticTrainError = (trainTable[1,2]+trainTable[2,1])/length(trainPrediction)





#Find the test error 
prob.training2 = round(predict(glm.fit, tst.cl, type="response"), digits=2)


trainPrediction2 = as.factor(ifelse(prob.training2<=0.5,  "Donald Trump", "Hillary Clinton"))
trainTable2 = table(trainPrediction2, t((select(tst.cl, candidate))))


kable(trainTable2, "html", caption = "logisticTestTable2", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)

logisticTestError = (trainTable2[1,2]+trainTable2[2,1])/length(trainPrediction2)



records[2,1] = logisticTrainError
records[2,2] = logisticTestError

```
    
  
  For logistic regression we found out that variables such as IncomePerCap, Professional, Service, Production, Drive, Carpool, Employed, PrivateWork and Unemployment are all relatively significant. They are mostly consistent but there are some inconsistency as well. For example variables such as Minority and Transit are significant variables in our decision tree model but not in our logistic regression model. 
  
  We can tell that when a given state or county gains 1 percent of white population, we expect an increase about e^0.1873 - 1 = .206 in the odds of Trump gets the final winning in this state or county. Likewise, if a given state or county gains 1 percent of employed population, we expect an increase about e^0.173 - 1 = .189 in the odds of Clinton gets the final winning in this state or county.  
  
  We also got training error rate of 0.0631 and test error rate of 0.0814.  
  


**18.  You may notice that you get a warning `glm.fit: fitted probabilities numerically 0 or 1 occurred`.**  As we discussed in class, this is an indication that we have perfect separation (some linear combination of variables _perfectly_ predicts the winner).  This is usually a sign that we are overfitting. One way to control overfitting in logistic regression is through regularization.  Use the `cv.glmnet` function from the `glmnet` library to run K-fold cross validation and select the best regularization parameter for the logistic regression with LASSO penalty.  Reminder: set `alpha=1` to run LASSO regression, set `lambda = c(1, 5, 10, 50) * 1e-4` in `cv.glmnet()` function to set pre-defined candidate values for the tuning parameter $\lambda$. This is because the default candidate values of $\lambda$ in `cv.glmnet()` is relatively too large for our dataset thus we use pre-defined candidate values. What is the optimal value of $\lambda$ in cross validation? What are the non-zero coefficients in the LASSO regression for the optimal value of $\lambda$? How do they compare to the unpenalized logistic regression?   Save training and test errors to the `records` variable.


```{r, cache =TRUE}
cv.out.lasso <- cv.glmnet(model.matrix(candidate ~., trn.cl)[,-1] , droplevels(trn.cl$candidate), alpha=1, lambda = c(1, 5, 10, 50) * 1e-4, family = "binomial")

bestLambda = cv.out.lasso$lambda.min


lasso.mod = glmnet(as.matrix(select(trn.cl, -candidate)), as.matrix(trn.cl$candidate), alpha=1, lambda=bestLambda , family = "binomial")


#Training error
lasso.pred = predict(lasso.mod, newx = as.matrix(select(trn.cl, -candidate)), s=bestLambda,  type = "response")

lassoTrainPrediction = as.factor(ifelse(lasso.pred<=0.5,  "Donald Trump", "Hillary Clinton"))
lassoTrainTable = table(lassoTrainPrediction, t((select(trn.cl, candidate))))
kable(lassoTrainTable, "html", caption = "lassoTrainTable", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)

lassoTrainError = (lassoTrainTable[1,2] + trainTable[2,1]) / length(lassoTrainPrediction)



#Test error
lasso.pred.test = predict(lasso.mod, newx = as.matrix(select(tst.cl, -candidate)), s=bestLambda,  type = "response")

lassoTestPrediction = as.factor(ifelse(lasso.pred.test<=0.5,  "Donald Trump", "Hillary Clinton"))
lassoTestTable = table(lassoTestPrediction, t((select(tst.cl, candidate))))


kable(lassoTestTable, "html", caption = "lassoTestTable", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)

lassoTestError = (lassoTestTable[1,2] + lassoTestTable[2,1]) / length(lassoTestPrediction)


records[3,1] = lassoTrainError
records[3,2] = lassoTestError

kable(records, caption = "records", "html", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)

#Check the non-zero coefficients
lasso.coef=predict(lasso.mod,type="coefficients",s=bestLambda)[1:26,]
kable(lasso.coef, "html", caption = "lasso coefs", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)
```
  The best lambda value for found the the regularization is 0.0005. 
  The only zero coefficiented pamameters in our lasso model are Minority and ChildPoverty.
  The coefficients we got are generally similar but less than those of the logistic regression.  
  
**19.  Compute ROC curves for the decision tree, logistic regression and LASSO logistic regression using predictions on the test data.**  Display them on the same plot.  Based on your classification results, discuss the pros and cons of the various methods.  Are the different classifiers more appropriate for answering different kinds of questions about the election?

```{r}
prd = predict(pt.prune, tst.cl, type="vector")
prd = prd[, colSums(prd != 0) > 0]
pred1 <- prediction(prd[,2], droplevels(tst.cl$candidate))
perf1 <- performance(pred1,"tpr","fpr")

pred2 <- prediction(predict(glm.fit, tst.cl, type="response"), droplevels(tst.cl$candidate))
perf2 <- performance(pred2,"tpr","fpr")

pred3 <- prediction(predict(lasso.mod, newx = as.matrix(select(tst.cl, -candidate)), s=bestLambda,  type = "response"),  droplevels(tst.cl$candidate))
perf3 <- performance(pred3,"tpr","fpr")

plot(perf1, col="red")
plot(perf2, add=TRUE, col = "blue")
plot(perf3, add=TRUE, col = "green")
legend('bottomright',  legend=c("tree", "logistic", "lasso"),
       col=c("red", "blue", "green"), lty=c(1,1), cex=0.8)
```
  
  Generally speaking, decision tree is one of the more visulizable and interpretable model that we can use for predicting election outcomes. It is not easily influenced by any  monotone transformations of predictors in the dataset. However, decision tree is very susceptible to small changes of data and therefore may be prone to overfitting. Decision tree is easier if we want to answer questions about how do we determine whether a candidate will win the election or not based on the magnitude of various of predictors.
  
  Logistic regression has some very nice properties when used for classification. We can see directly how much weights does our classifier puts on each of the predicting parameters. But it does not perform well when the feature space is too large nor when there are many categorical data present. It is also susceptible to the perfect separation problem which we did encounter when we were fitting our model. Logistic regression can be used to answer probalistic questions such as given a certain set of preditors, what are the probability of a certain candidate will win the election.
  
  Lasso logistic regression is a improved version of our logistic regression by using regularization technique and therefore significantly reduces the chance of overfitting. It assigns more parameter zero coefficients so we can quickly tell which are the more important predictors. Nonetheless, since it is an automated process, it might happen that lasso eliminates some predictors containing vital information. So we might not always get the most significant predictors for our model. Similar to logistic regression, lasso logistic regression can answer probalistic questions.
  

# Taking it further

**20. This is an open question. Interpret and discuss any overall insights gained in this analysis and possible explanations.** Use any tools at your disposal to make your case: visualize errors on the map, discuss what does/doesn't seems reasonable based on your understanding of these methods, propose possible directions (collecting additional data, domain knowledge, etc).  In addition, propose and tackle _at least_ one more interesting question. Creative and thoughtful analyses will be rewarded! _This part will be worth up to a 20\% of your final project grade!_  

Some possibilities for further exploration are:

  * Data preprocessing: we aggregated sub-county level data before performing classification. Would classification at the sub-county level before determining the winner perform better? What implicit assumptions are we making?

  * Exploring additional classification methods: KNN, LDA, QDA, SVM, random forest, boosting etc. (You may research and use methods beyond those covered in this course). How do these compare to logistic regression and the tree method?

  * Bootstrap: Perform boostrap to generate plots similar to ISLR Figure 4.10/4.11. Discuss the results. 
  
  * Use linear regression models to predict the `total` vote for each candidate by county.  Compare and contrast these results with the classification models.  Which do you prefer and why?  How might they complement one another?
    
  * Conduct an exploratory analysis of the "purple" counties-- the counties which the models predict Clinton and Trump were roughly equally likely to win.  What is it about these counties that make them hard to predict?
    
  * Instead of using the native attributes (the original features), we can use principal components to create new (and lower dimensional) set of features with which to train a classification model.  This sometimes improves classification performance.  Compare classifiers trained on the original features with those trained on PCA features.  

__Random Forest__:  

  We want to further modify or improve the methods we used previously. One way we thought can could enhance our model accuracy is to turn the decision tree into a random forest. Since decision tree usually sufferes from high variance problem, aggregating them into random forest could lead us to some better results.  
  
  
```{r}
rf.election = randomForest(candidate ~ ., data=droplevels(trn.cl), mtry=5, ntree=500, importance=TRUE)


# calculate the trainnig and test errors
rfTrainTable = rf.election$confusion
kable(rfTrainTable, caption = "rfTrainTable", "html", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)

rfTrainError = (rfTrainTable[1,2]+rfTrainTable[2,1]) / length(trn.cl)



rfTest = predict(rf.election, tst.cl, type="response")
rfTestTable = table(rfTest, t((select(tst.cl, candidate))))
kable(rfTestTable, caption = "rfTestTable", "html", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)


rfTestError = (rfTestTable[1,2] + rfTestTable[2,1])/length(rfTest)


```
  We found that the test error of our random forest is about 0.0732899, which is a significant improvement form decisoin tree(0.09771987). By aggregating 500 decision trees, random forest methods has effectively reduce the high variance problem that suffered by decision tree. 
  
__SVM__:  

  We also decide to use support vector machine classifier to predict the outcomes. It was considered to be the best "off the shelf" classifier and it serves well for binary classification.  
  
  
```{r}


#Tuning for the cost parameter 
set.seed(1)

svmfit=svm(candidate~., data=droplevels(trn.cl), kernel="linear", cost=0.1, scale=T)

#Training error
svm.pred = predict(svmfit, data = trn.cl)

#svmTrainPrediction = as.factor(ifelse(svm.pred<=0.5,  "Donald Trump", "Hillary Clinton"))
svmTrainTable = table(svm.pred, t((select(trn.cl, candidate))))
kable(svmTrainTable, "html", caption = "svmTrainTable", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)

svmTrainError = (svmTrainTable[1,2] + svmTrainTable[2,1]) / length(svm.pred)


#Test error
svm.pred.test = predict(svmfit, tst.cl)

#svmTestPrediction = as.factor(ifelse(svm.pred.test<=0.5,  "Donald Trump", "Hillary Clinton"))
svmTestTable = table(svm.pred.test, t((select(tst.cl, candidate))))
kable(svmTestTable, "html", caption = "svmTestTable", booktabs = T, align = c("c","c")) %>% 
  kable_styling(position = "center", full_width = F)


svmTestError = (svmTestTable[1,2] + svmTestTable[2,1]) / length(svm.pred.test)

```

  The SVM does not really gain us much predicting power since its test error is 0.08143322 which is similar to that of the lasso logistic regression. 


